{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4f7d253239fcee8be6d4237301db7fec68dc2f84"
   },
   "source": [
    "# Starter with Neural Networks\n",
    "There is almost no work with features, I only split the datetime column into 6 columns, one-hot encoded 'passenger_count', extracted order ID from 'key' and used two features from the baseline kernel. The model is flawed and not tuned at all, its only purpose was to make sure that loss goes down no matter what, hence dropout+L2+BN. I almost purposefully made a bunch of mistakes in hope that somebody publicly corrects them.\n",
    "\n",
    "Despite all that, I achieved 3.95 MSE with 10M samples and 3.83 MSE with all data. There is plenty of work ahead, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "9aaef520e80325dcd8a8b722d57be357fe3d2794"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Initial Python environment setup...\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # CSV file I/O (e.g. pd.read_csv)\n",
    "import os # reading the input files we have access to\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras_tqdm import TQDMNotebookCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "382537c3ee86798718d397559af241512b19dd9e"
   },
   "outputs": [],
   "source": [
    "#features from basic linear model kernel\n",
    "def add_travel_vector_features(df):\n",
    "    df['abs_diff_longitude'] = (df.dropoff_longitude - df.pickup_longitude).abs()\n",
    "    df['abs_diff_latitude'] = (df.dropoff_latitude - df.pickup_latitude).abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d3de21aba974460653a58e3fb389873ff76eef02"
   },
   "source": [
    "# Loading and preprocessing data in its entirety\n",
    "I managed to load and preprocess the whole dataset with pandas, but it took ~20 minutes. Again, I'm uploading it so that somebody shows how to do it correctly with, I dunno, Dask. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "6a3e28f02f27b273ebe97450fb1986a136388637"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56it [13:45, 14.74s/it]\n"
     ]
    }
   ],
   "source": [
    "filename = 'data/nyc-taxi/train.csv'\n",
    "dfs = []\n",
    "chunksize = 10 ** 6\n",
    "for chunk in tqdm(pd.read_csv(filename, chunksize=chunksize)):\n",
    "    #preprocessing section\n",
    "    add_travel_vector_features(chunk)\n",
    "    chunk = chunk.dropna(how = 'any', axis = 'rows')\n",
    "    chunk = chunk[(chunk.abs_diff_longitude < 5.0) & (chunk.abs_diff_latitude < 5.0)]\n",
    "    chunk = chunk[(chunk.passenger_count > 0) & (chunk.passenger_count <= 6)]\n",
    "    chunk[['date','time','timezone']] = chunk['pickup_datetime'].str.split(expand=True)\n",
    "    chunk[['year','month','day']] = chunk['date'].str.split('-',expand=True).astype('int64')\n",
    "    chunk[['hour','minute','second']] = chunk['time'].str.split(':',expand=True).astype('int64')\n",
    "    chunk['year_after_0'] = chunk['year'] - np.min(chunk['year'])\n",
    "    chunk[['trash', 'order_no']] = chunk['key'].str.split('.',expand=True)\n",
    "    chunk['order_no'] = chunk['order_no'].astype('int64')\n",
    "    chunk = pd.concat([chunk,pd.get_dummies(chunk['passenger_count'],prefix='pass')], axis =1)\n",
    "    chunk = chunk.drop(['timezone','date','time', 'pickup_datetime','trash','key','passenger_count'], axis = 1)\n",
    "    #append chunk to the list\n",
    "    dfs.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "7c47ee53805734eb29d4eec0f3aaa78f900c4a55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#concatenate all chunk in one big-ass DataFrame\n",
    "train_df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "8dad9fccf9d07e689324e3718c281defabbbb830"
   },
   "outputs": [],
   "source": [
    "#delete the chunks as I only have 16 GB RAM\n",
    "del dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "55663585a864273d0216eded72168eef66076127"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>abs_diff_longitude</th>\n",
       "      <th>abs_diff_latitude</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>...</th>\n",
       "      <th>minute</th>\n",
       "      <th>second</th>\n",
       "      <th>year_after_0</th>\n",
       "      <th>order_no</th>\n",
       "      <th>pass_1</th>\n",
       "      <th>pass_2</th>\n",
       "      <th>pass_3</th>\n",
       "      <th>pass_4</th>\n",
       "      <th>pass_5</th>\n",
       "      <th>pass_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.5</td>\n",
       "      <td>-73.844311</td>\n",
       "      <td>40.721319</td>\n",
       "      <td>-73.841610</td>\n",
       "      <td>40.712278</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>0.009041</td>\n",
       "      <td>2009</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16.9</td>\n",
       "      <td>-74.016048</td>\n",
       "      <td>40.711303</td>\n",
       "      <td>-73.979268</td>\n",
       "      <td>40.782004</td>\n",
       "      <td>0.036780</td>\n",
       "      <td>0.070701</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>52</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.7</td>\n",
       "      <td>-73.982738</td>\n",
       "      <td>40.761270</td>\n",
       "      <td>-73.991242</td>\n",
       "      <td>40.750562</td>\n",
       "      <td>0.008504</td>\n",
       "      <td>0.010708</td>\n",
       "      <td>2011</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>...</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.7</td>\n",
       "      <td>-73.987130</td>\n",
       "      <td>40.733143</td>\n",
       "      <td>-73.991567</td>\n",
       "      <td>40.758092</td>\n",
       "      <td>0.004437</td>\n",
       "      <td>0.024949</td>\n",
       "      <td>2012</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.3</td>\n",
       "      <td>-73.968095</td>\n",
       "      <td>40.768008</td>\n",
       "      <td>-73.956655</td>\n",
       "      <td>40.783762</td>\n",
       "      <td>0.011440</td>\n",
       "      <td>0.015754</td>\n",
       "      <td>2010</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>135</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   fare_amount  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "0          4.5        -73.844311        40.721319         -73.841610   \n",
       "1         16.9        -74.016048        40.711303         -73.979268   \n",
       "2          5.7        -73.982738        40.761270         -73.991242   \n",
       "3          7.7        -73.987130        40.733143         -73.991567   \n",
       "4          5.3        -73.968095        40.768008         -73.956655   \n",
       "\n",
       "   dropoff_latitude  abs_diff_longitude  abs_diff_latitude  year  month  day  \\\n",
       "0         40.712278            0.002701           0.009041  2009      6   15   \n",
       "1         40.782004            0.036780           0.070701  2010      1    5   \n",
       "2         40.750562            0.008504           0.010708  2011      8   18   \n",
       "3         40.758092            0.004437           0.024949  2012      4   21   \n",
       "4         40.783762            0.011440           0.015754  2010      3    9   \n",
       "\n",
       "    ...    minute  second  year_after_0  order_no  pass_1  pass_2  pass_3  \\\n",
       "0   ...        26      21             0         1       1       0       0   \n",
       "1   ...        52      16             1         2       1       0       0   \n",
       "2   ...        35       0             2        49       0       1       0   \n",
       "3   ...        30      42             3         1       1       0       0   \n",
       "4   ...        51       0             1       135       1       0       0   \n",
       "\n",
       "   pass_4  pass_5  pass_6  \n",
       "0       0       0       0  \n",
       "1       0       0       0  \n",
       "2       0       0       0  \n",
       "3       0       0       0  \n",
       "4       0       0       0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "0cd6c16d2549289f2962023dcc2521bf5be889ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55115115, 21)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 55115115 entries, 0 to 55423855\n",
      "Data columns (total 21 columns):\n",
      "fare_amount           float64\n",
      "pickup_longitude      float64\n",
      "pickup_latitude       float64\n",
      "dropoff_longitude     float64\n",
      "dropoff_latitude      float64\n",
      "abs_diff_longitude    float64\n",
      "abs_diff_latitude     float64\n",
      "year                  int64\n",
      "month                 int64\n",
      "day                   int64\n",
      "hour                  int64\n",
      "minute                int64\n",
      "second                int64\n",
      "year_after_0          int64\n",
      "order_no              int64\n",
      "pass_1                uint8\n",
      "pass_2                uint8\n",
      "pass_3                uint8\n",
      "pass_4                uint8\n",
      "pass_5                uint8\n",
      "pass_6                uint8\n",
      "dtypes: float64(7), int64(8), uint8(6)\n",
      "memory usage: 6.9 GB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "05a992e50f8af153ef5bfd3f70b5d344f19c178e"
   },
   "outputs": [],
   "source": [
    "X_train = train_df.drop(['fare_amount'],axis=1)\n",
    "Y_train = train_df['fare_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "1022206873eff972325bd13f6dfdd02e2791b35a"
   },
   "outputs": [],
   "source": [
    "del train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "a1a9b27b1905447d9cf6ae7719352fa6bef1a54c"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "334167c6ec185a9923afa8712b36d35c90bc8f54"
   },
   "outputs": [],
   "source": [
    "#scale the data so that columns have zero mean and unit variance\n",
    "train = scaler.fit_transform(X_train.values)\n",
    "y_train =  y_scaler.fit_transform(Y_train.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "c7ff6e7bfb2818cd8d3154029b1d34e1df4f777d"
   },
   "outputs": [],
   "source": [
    "del X_train\n",
    "del Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "f0bb3eeb22360a83d7f08fd3ae94e82981d48edd"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "c78b7cee111fb0fb27ec95d1ab2ea2ac5d24fd64"
   },
   "outputs": [],
   "source": [
    "#some imports are unnecessary\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dropout,Dense, Activation, BatchNormalization\n",
    "from keras.models import Model, load_model\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.callbacks import ModelCheckpoint,  ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "903817b654aad0aa7ae432f20d39d36219810b73"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "24e03c4adcb0230e095dd1c292f71499ce0370d3"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(1024,kernel_initializer = glorot_uniform(),\n",
    "              kernel_regularizer = l2(1e-2)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(tf.nn.leaky_relu),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1024,kernel_initializer = glorot_uniform(),\n",
    "              kernel_regularizer = l2(1e-2)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(tf.nn.leaky_relu),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1024,kernel_initializer = glorot_uniform(),\n",
    "              kernel_regularizer = l2(1e-2)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(tf.nn.leaky_relu),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(1024,kernel_initializer = glorot_uniform(),\n",
    "              kernel_regularizer = l2(1e-2)),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(tf.nn.leaky_relu),\n",
    "    keras.layers.Dense(1, activation=tf.nn.leaky_relu)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "ac21ef3ce5aeca5b35a9cc68e1306114b74dbac4"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(5e-4), \n",
    "              loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "57a95c9078d54a9f6bf954940a105243d2cc81af"
   },
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "20ff141bcc075e70500d6beb2e3d5fda71eebece"
   },
   "outputs": [],
   "source": [
    "filepath = './model_weights/weights-improvement-55M-{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "best_callback = ModelCheckpoint(filepath, \n",
    "                                save_best_only=True)\n",
    "lr_sched = ReduceLROnPlateau(monitor='val_loss', factor = 0.2, patience = 5, verbose = 1)\n",
    "tqdm_callback = TQDMNotebookCallback(leave_inner=True,metric_format=\"{name}: {value:0.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "999d867960b261f84aa319814c55dc3ea9c058b2"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "95da09eeac11c214dfe3a482441b7128650628d8"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train, y_train, \n",
    "          epochs=20,\n",
    "          verbose=0,\n",
    "          batch_size=2048,\n",
    "          validation_split=0.0002,\n",
    "          callbacks=[tqdm_callback,best_callback, lr_sched])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d9d494754f297775ebd484ed4be7b7d7183c1496"
   },
   "source": [
    "# Load best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "739228764cfe67b53041533f57be28919e5f0d3f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('./model_weights/weights-improvement-55M-19-0.0471.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "511ef0b8909f5364215529fc5c46817f9ab3728f"
   },
   "source": [
    "# Load and preprocess test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ac25bde9f00a5917b666434c269504b02df39d59",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "test_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a600ad8a0d88e68a638b9cfd20998d42c8b236b9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key = test_df.key\n",
    "add_travel_vector_features(test_df)\n",
    "test_df[['date','time','timezone']] = test_df['pickup_datetime'].str.split(expand=True)\n",
    "test_df[['year','month','day']] = test_df['date'].str.split('-',expand=True).astype('int64')\n",
    "test_df[['hour','minute','second']] = test_df['time'].str.split(':',expand=True).astype('int64')\n",
    "test_df['year_after_0'] = test_df['year'] - np.min(test_df['year'])\n",
    "test_df[['trash', 'order_no']] = test_df['key'].str.split('.',expand=True)\n",
    "test_df['order_no'] = test_df['order_no'].astype('int64')\n",
    "test_df = pd.concat([test_df,pd.get_dummies(test_df['passenger_count'],prefix='pass')], axis =1)\n",
    "test_df = test_df.drop(['timezone','date','time', 'pickup_datetime','trash','key','passenger_count'], axis = 1)\n",
    "# Predict fare_amount on the test set using our model (w) tested on the testing set.\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7c0c4005d5c823dca05f7ae2eef701e004f41736"
   },
   "source": [
    "# Inference and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "055a9b1581bdff1b6bcfcc7493b872e73a9bb416",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = scaler.transform(test_df.values)\n",
    "y_test = model.predict(test)\n",
    "y_test = y_scaler.inverse_transform(y_test).reshape(-1)\n",
    "# Write the predictions to a CSV file which we can submit to the competition.\n",
    "submission = pd.DataFrame(\n",
    "    {'key': key, 'fare_amount': y_test},\n",
    "    columns = ['key', 'fare_amount'])\n",
    "submission.to_csv('submission_100.csv', index = False)\n",
    "\n",
    "print(os.listdir('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f0940b90544b1233d07e762569f6ea92cfbf611c"
   },
   "source": [
    "# What's next\n",
    "1. Extract better features.\n",
    "2. Choose a better architecture.\n",
    "3. Tune the hyperparameters.\n",
    "4. Forget all that and resort to XGBoost and ensembling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "33228044841d46a5311307af13dd5c6e4d5ddc95",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
