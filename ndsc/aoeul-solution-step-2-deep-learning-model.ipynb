{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network architecture is based on https://www.kaggle.com/artgor/movie-review-sentiment-analysis-eda-and-models \n",
    "\n",
    "Actually this model is an overkill for this competition and takes too long (8 hours!) to train and predict. This is what happened when you copied someone's code without knowing (or assumed wrongly) what you were actually doing.\n",
    "\n",
    "Team yellow's simple feedforward neural network (https://www.kaggle.com/astraldawn/yellow-feedforward-neural-network) is much better in terms of speed and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "start = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('2.2.4-tf', '1.13.1')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, BatchNormalization, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import Input, GlobalMaxPool1D, Conv1D, MaxPooling1D, GRU, concatenate, CuDNNGRU\n",
    "from keras.layers import LSTM, CuDNNLSTM,  Bidirectional, SpatialDropout1D\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\n",
    "from keras import utils\n",
    "from keras.utils import to_categorical\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "keras.__version__, tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 'Deep_Learning_predictions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('data')\n",
    "\n",
    "BEAUTY_JSON = DATA_DIR / 'beauty_profile_train.json'\n",
    "FASHION_JSON = DATA_DIR / 'fashion_profile_train.json'\n",
    "MOBILE_JSON = DATA_DIR / 'mobile_profile_train.json'\n",
    "\n",
    "BEAUTY_TRAIN_CSV = DATA_DIR / 'beauty_data_info_train_competition.csv'\n",
    "FASHION_TRAIN_CSV = DATA_DIR / 'fashion_data_info_train_competition.csv'\n",
    "MOBILE_TRAIN_CSV = DATA_DIR / 'mobile_data_info_train_competition.csv'\n",
    "\n",
    "BEAUTY_TEST_CSV = DATA_DIR / 'beauty_data_info_val_competition.csv'\n",
    "FASHION_TEST_CSV = DATA_DIR / 'fashion_data_info_val_competition.csv'\n",
    "MOBILE_TEST_CSV = DATA_DIR / 'mobile_data_info_val_competition.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BEAUTY_JSON) as f:\n",
    "     beauty_attribs = json.load(f)\n",
    "        \n",
    "with open(FASHION_JSON) as f:\n",
    "     fashion_attribs = json.load(f)\n",
    "        \n",
    "with open(MOBILE_JSON) as f:\n",
    "     mobile_attribs = json.load(f)\n",
    "\n",
    "beauty_train_df = pd.read_csv(BEAUTY_TRAIN_CSV)\n",
    "fashion_train_df = pd.read_csv(FASHION_TRAIN_CSV)\n",
    "mobile_train_df = pd.read_csv(MOBILE_TRAIN_CSV)\n",
    "\n",
    "beauty_test_df = pd.read_csv(BEAUTY_TEST_CSV)\n",
    "fashion_test_df = pd.read_csv(FASHION_TEST_CSV)\n",
    "mobile_test_df = pd.read_csv(MOBILE_TEST_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(286583, 275142, 160330)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(beauty_train_df), len(fashion_train_df), len(mobile_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76545, 30135, 40417)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(beauty_test_df), len(fashion_test_df), len(mobile_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "977987\n"
     ]
    }
   ],
   "source": [
    "n_rows = len(beauty_test_df)*5 + len(fashion_test_df)*5 + len(mobile_test_df)*11 \n",
    "print(n_rows)\n",
    "assert n_rows == 977987, \"Row numbers don't match!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words_beauty = 29\n",
    "max_words_fashion = 32\n",
    "max_words_mobile = 27\n",
    "batch_size = 32\n",
    "epochs_beauty = [25, 25, 25, 25, 25]\n",
    "epochs_fashion = [20, 20, 20, 20, 20]\n",
    "epochs_mobile = [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]\n",
    "embed_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(max_features, max_len, num_classes, lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, \n",
    "                 kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
    "    inp = Input(shape=(max_len,))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences=True))(x1)\n",
    "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences=True))(x1)\n",
    "\n",
    "    x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n",
    "    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n",
    "\n",
    "    x_conv2 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "    avg_pool2_gru = GlobalAveragePooling1D()(x_conv2)\n",
    "    max_pool2_gru = GlobalMaxPooling1D()(x_conv2)\n",
    "\n",
    "    x_conv3 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool1_lstm = GlobalAveragePooling1D()(x_conv3)\n",
    "    max_pool1_lstm = GlobalMaxPooling1D()(x_conv3)\n",
    "\n",
    "    x_conv4 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "    avg_pool2_lstm = GlobalAveragePooling1D()(x_conv4)\n",
    "    max_pool2_lstm = GlobalMaxPooling1D()(x_conv4)\n",
    "\n",
    "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool2_gru, max_pool2_gru,\n",
    "                     avg_pool1_lstm, max_pool1_lstm, avg_pool2_lstm, max_pool2_lstm])\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(dense_units, activation='relu')(x))\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dr)(Dense(int(dense_units/2), activation='relu')(x))\n",
    "    x = Dense(units=num_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=[\"accuracy\"])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_model(max_features, num_classes, maxlen):    \n",
    "    model = build_model(max_features, maxlen, num_classes,\n",
    "                         lr = 1e-4, lr_d = 0, units = 64, spatial_dr = 0.5, \n",
    "                         kernel_size1=4, kernel_size2=3, dense_units=32, dr=0.1, conv_size=32)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beauty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total attributes for beauty products: 5\n",
      "\n",
      "Attribute_1: Benefits (7 classes)\n",
      "Attribute_2: Brand (401 classes)\n",
      "Attribute_3: Colour_group (45 classes)\n",
      "Attribute_4: Product_texture (9 classes)\n",
      "Attribute_5: Skin_type (8 classes)\n",
      "\n",
      "Total 29506 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print(f'Total attributes for beauty products: {len(beauty_attribs)}')\n",
    "print()\n",
    "\n",
    "for i, attrib in enumerate(beauty_attribs, 1):\n",
    "    print(f'Attribute_{i}: {attrib} ({len(beauty_attribs[attrib])} classes)')\n",
    "\n",
    "all_beauty_titles = list(beauty_train_df.title)\n",
    "beauty_tokenizer = Tokenizer()\n",
    "beauty_tokenizer.fit_on_texts(all_beauty_titles)\n",
    "tokenized_train_titles = beauty_tokenizer.texts_to_sequences(list(beauty_train_df.title))\n",
    "print()\n",
    "print(f'Total {len(beauty_tokenizer.word_index)} unique tokens.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Attrib_1 Benefits]: (7 classes)\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "\n",
      "[Attrib_2 Brand]: (401 classes)\n",
      "\n",
      "[Attrib_3 Colour_group]: (45 classes)\n",
      "\n",
      "[Attrib_4 Product_texture]: (9 classes)\n",
      "\n",
      "[Attrib_5 Skin_type]: (8 classes)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "map_beauty = []\n",
    "for i, attrib in enumerate(beauty_attribs):\n",
    "    num_classes = len(beauty_attribs[attrib])\n",
    "    print(f'[Attrib_{i+1} {attrib}]: ({num_classes} classes)')\n",
    "    beauty_attrib_train_df = beauty_train_df[['title', attrib]].dropna()\n",
    "    titles = list(beauty_attrib_train_df.title)\n",
    "    tokenized_train_titles = beauty_tokenizer.texts_to_sequences(titles)\n",
    "\n",
    "    X = np.array(tokenized_train_titles)\n",
    "    X = sequence.pad_sequences(X, padding='post', maxlen=max_words_beauty)\n",
    "    y = to_categorical(beauty_attrib_train_df[attrib], num_classes)\n",
    "    \n",
    "    max_features = len(beauty_tokenizer.word_index) # how many unique words to use\n",
    "    model = RNN_model(max_features, num_classes, max_words_beauty)\n",
    "    \n",
    "    file_path = f\"{VERSION}_model_beauty_{attrib}.hdf5\"\n",
    "    \n",
    "    hist = model.fit(X, y, \n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs_beauty[i],\n",
    "                    verbose=0)\n",
    "    model.save(file_path)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total seconds: 9768.316558\n",
      "Total minutes: 162.81\n",
      "Total hour: 2.71\n"
     ]
    }
   ],
   "source": [
    "duration = datetime.now()-start\n",
    "print('Total seconds:', duration.total_seconds())\n",
    "print(f'Total minutes: {duration.total_seconds()/60:.2f}')\n",
    "print(f'Total hour: {duration.total_seconds()/3600:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total attributes for fashion products: 5\n",
      "\n",
      "Attribute_1: Pattern (20 classes)\n",
      "Attribute_2: Collar Type (16 classes)\n",
      "Attribute_3: Fashion Trend (11 classes)\n",
      "Attribute_4: Clothing Material (19 classes)\n",
      "Attribute_5: Sleeves (4 classes)\n",
      "\n",
      "Total 45770 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print(f'Total attributes for fashion products: {len(fashion_attribs)}')\n",
    "print()\n",
    "\n",
    "for i, attrib in enumerate(fashion_attribs, 1):\n",
    "    print(f'Attribute_{i}: {attrib} ({len(fashion_attribs[attrib])} classes)')\n",
    "\n",
    "all_fashion_titles = list(fashion_train_df.title)\n",
    "fashion_tokenizer = Tokenizer()\n",
    "fashion_tokenizer.fit_on_texts(all_fashion_titles)\n",
    "tokenized_train_titles = fashion_tokenizer.texts_to_sequences(list(fashion_train_df.title))\n",
    "print()\n",
    "print(f'Total {len(fashion_tokenizer.word_index)} unique tokens.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Attrib_1 Pattern]: (20 classes)\n",
      "\n",
      "[Attrib_2 Collar Type]: (16 classes)\n",
      "\n",
      "[Attrib_3 Fashion Trend]: (11 classes)\n",
      "\n",
      "[Attrib_4 Clothing Material]: (19 classes)\n",
      "\n",
      "[Attrib_5 Sleeves]: (4 classes)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "map_fashion = []\n",
    "for i, attrib in enumerate(fashion_attribs):\n",
    "    num_classes = len(fashion_attribs[attrib])\n",
    "    print(f'[Attrib_{i+1} {attrib}]: ({num_classes} classes)')\n",
    "    fashion_attrib_train_df = fashion_train_df[['title', attrib]].dropna()\n",
    "    titles = list(fashion_attrib_train_df.title)\n",
    "    tokenized_train_titles = fashion_tokenizer.texts_to_sequences(titles)\n",
    "\n",
    "    X = np.array(tokenized_train_titles)\n",
    "    X = sequence.pad_sequences(X, padding='post', maxlen=max_words_fashion)\n",
    "    y = to_categorical(fashion_attrib_train_df[attrib], num_classes)\n",
    "    \n",
    "    max_features = len(fashion_tokenizer.word_index) # how many unique words to use\n",
    "    model = RNN_model(max_features, num_classes, max_words_fashion)\n",
    "    \n",
    "    file_path = f\"{VERSION}_model_fashion_{attrib}.hdf5\"\n",
    "    \n",
    "    hist = model.fit(X, y, \n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs_fashion[i],\n",
    "                    verbose=0)\n",
    "    model.save(file_path)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total seconds: 18575.284479\n",
      "Total minutes: 309.59\n",
      "Total hour: 5.16\n"
     ]
    }
   ],
   "source": [
    "duration = datetime.now()-start\n",
    "print('Total seconds:', duration.total_seconds())\n",
    "print(f'Total minutes: {duration.total_seconds()/60:.2f}')\n",
    "print(f'Total hour: {duration.total_seconds()/3600:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total attributes for mobile products: 11\n",
      "\n",
      "Attribute_1: Operating System (7 classes)\n",
      "Attribute_2: Features (7 classes)\n",
      "Attribute_3: Network Connections (4 classes)\n",
      "Attribute_4: Memory RAM (10 classes)\n",
      "Attribute_5: Brand (56 classes)\n",
      "Attribute_6: Warranty Period (14 classes)\n",
      "Attribute_7: Storage Capacity (18 classes)\n",
      "Attribute_8: Color Family (26 classes)\n",
      "Attribute_9: Phone Model (2280 classes)\n",
      "Attribute_10: Camera (15 classes)\n",
      "Attribute_11: Phone Screen Size (6 classes)\n",
      "\n",
      "Total 24529 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "print(f'Total attributes for mobile products: {len(mobile_attribs)}')\n",
    "print()\n",
    "\n",
    "for i, attrib in enumerate(mobile_attribs, 1):\n",
    "    print(f'Attribute_{i}: {attrib} ({len(mobile_attribs[attrib])} classes)')\n",
    "\n",
    "all_mobile_titles = list(mobile_train_df.title)\n",
    "mobile_tokenizer = Tokenizer()\n",
    "mobile_tokenizer.fit_on_texts(all_mobile_titles)\n",
    "tokenized_train_titles = mobile_tokenizer.texts_to_sequences(list(mobile_train_df.title))\n",
    "print()\n",
    "print(f'Total {len(mobile_tokenizer.word_index)} unique tokens.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Attrib_1 Operating System]: (7 classes)\n",
      "\n",
      "[Attrib_2 Features]: (7 classes)\n",
      "\n",
      "[Attrib_3 Network Connections]: (4 classes)\n",
      "\n",
      "[Attrib_4 Memory RAM]: (10 classes)\n",
      "\n",
      "[Attrib_5 Brand]: (56 classes)\n",
      "\n",
      "[Attrib_6 Warranty Period]: (14 classes)\n",
      "\n",
      "[Attrib_7 Storage Capacity]: (18 classes)\n",
      "\n",
      "[Attrib_8 Color Family]: (26 classes)\n",
      "\n",
      "[Attrib_9 Phone Model]: (2280 classes)\n",
      "\n",
      "[Attrib_10 Camera]: (15 classes)\n",
      "\n",
      "[Attrib_11 Phone Screen Size]: (6 classes)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "map_mobile = []\n",
    "for i, attrib in enumerate(mobile_attribs):\n",
    "    num_classes = len(mobile_attribs[attrib])\n",
    "    print(f'[Attrib_{i+1} {attrib}]: ({num_classes} classes)')\n",
    "    mobile_attrib_train_df = mobile_train_df[['title', attrib]].dropna()\n",
    "    titles = list(mobile_attrib_train_df.title)\n",
    "    tokenized_train_titles = mobile_tokenizer.texts_to_sequences(titles)\n",
    "\n",
    "    X = np.array(tokenized_train_titles)\n",
    "    X = sequence.pad_sequences(X, padding='post', maxlen=max_words_mobile)\n",
    "    y = to_categorical(mobile_attrib_train_df[attrib], num_classes)\n",
    "    \n",
    "    max_features = len(mobile_tokenizer.word_index) # how many unique words to use\n",
    "    model = RNN_model(max_features, num_classes, max_words_mobile)\n",
    "    \n",
    "    file_path = f\"{VERSION}_model_mobile_{attrib}.hdf5\"\n",
    "    \n",
    "    hist = model.fit(X, y, \n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs_mobile[i],\n",
    "                    verbose=0)\n",
    "    model.save(file_path)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total seconds: 25631.789211\n",
      "Total minutes: 427.20\n",
      "Total hour: 7.12\n"
     ]
    }
   ],
   "source": [
    "duration = datetime.now()-start\n",
    "print('Total seconds:', duration.total_seconds())\n",
    "print(f'Total minutes: {duration.total_seconds()/60:.2f}')\n",
    "print(f'Total hour: {duration.total_seconds()/3600:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute: Benefits\n",
      "Loading model...\n",
      "Predicting...\n",
      "Sorting predictions...\n",
      "\n",
      "Attribute: Brand\n",
      "Loading model...\n",
      "Predicting...\n",
      "Sorting predictions...\n",
      "\n",
      "Attribute: Colour_group\n",
      "Loading model...\n",
      "Predicting...\n",
      "Sorting predictions...\n",
      "\n",
      "Attribute: Product_texture\n",
      "Loading model...\n",
      "Predicting...\n",
      "Sorting predictions...\n",
      "\n",
      "Attribute: Skin_type\n",
      "Loading model...\n",
      "Predicting...\n",
      "Sorting predictions...\n",
      "\n",
      "Saving to Deep_Learning_predictions_pred_beauty.csv\n",
      "CPU times: user 4min 17s, sys: 5.35 s, total: 4min 22s\n",
      "Wall time: 4min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "updated_beauty_title = list(beauty_test_df.title)\n",
    "tokenized_test_titles = beauty_tokenizer.texts_to_sequences(updated_beauty_title)\n",
    "X_test = np.array(tokenized_test_titles)\n",
    "X_test = sequence.pad_sequences(X_test, padding='post', maxlen=max_words_beauty)\n",
    "\n",
    "preds_all={}\n",
    "for attrib in beauty_attribs:\n",
    "    print(f'Attribute: {attrib}')\n",
    "    print(f'Loading model...')\n",
    "    model=load_model(f'{VERSION}_model_beauty_{attrib}.hdf5')\n",
    "    print(f'Predicting...')\n",
    "    pred_attrib=model.predict(X_test)\n",
    "    print(f'Sorting predictions...')\n",
    "    pred_list=[]\n",
    "    for pred in pred_attrib:\n",
    "        pred_list.append(pred.argsort()[-2:][::-1])\n",
    "    preds_all[attrib]=np.array(pred_list.copy())\n",
    "    print()\n",
    "    \n",
    "print(f'Saving to {VERSION}_pred_beauty.csv')\n",
    "test_y_id=[] \n",
    "test_y_predictions=[]\n",
    "for i, itemid in enumerate(beauty_test_df.itemid):\n",
    "    for attrib in beauty_attribs:\n",
    "        test_y_id.append(str(itemid) + f'_{attrib}')\n",
    "        test_y_predictions.append(str(preds_all[attrib][i][0]) + ' ' + str(preds_all[attrib][i][1]))\n",
    "        \n",
    "beauty_result_df = pd.DataFrame(\n",
    "    {'id': test_y_id, 'tagging': test_y_predictions},\n",
    "    columns = ['id', 'tagging'])\n",
    "\n",
    "beauty_result_df.to_csv(f'{VERSION}_pred_beauty.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute: Pattern\n",
      "Loading model...\n",
      "Predicting...\n",
      "Sorting predictions...\n",
      "\n",
      "Attribute: Collar Type\n",
      "Loading model...\n",
      "Predicting...\n",
      "Sorting predictions...\n",
      "\n",
      "Attribute: Fashion Trend\n",
      "Loading model...\n",
      "Predicting...\n",
      "Sorting predictions...\n",
      "\n",
      "Attribute: Clothing Material\n",
      "Loading model...\n",
      "Predicting...\n",
      "Sorting predictions...\n",
      "\n",
      "Attribute: Sleeves\n",
      "Loading model...\n",
      "Predicting...\n",
      "Sorting predictions...\n",
      "\n",
      "Saving to Deep_Learning_predictions_pred_fashion.csv\n",
      "CPU times: user 4min 32s, sys: 4.78 s, total: 4min 36s\n",
      "Wall time: 4min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "updated_fashion_title = list(fashion_test_df.title)\n",
    "tokenized_test_titles = fashion_tokenizer.texts_to_sequences(updated_fashion_title)\n",
    "X_test = np.array(tokenized_test_titles)\n",
    "X_test = sequence.pad_sequences(X_test, padding='post', maxlen=max_words_fashion)\n",
    "\n",
    "preds_all={}\n",
    "for attrib in fashion_attribs:\n",
    "    print(f'Attribute: {attrib}')\n",
    "    print(f'Loading model...')\n",
    "    model=load_model(f'{VERSION}_model_fashion_{attrib}.hdf5')\n",
    "    print(f'Predicting...')\n",
    "    pred_attrib=model.predict(X_test)\n",
    "    print(f'Sorting predictions...')\n",
    "    pred_list=[]\n",
    "    for pred in pred_attrib:\n",
    "        pred_list.append(pred.argsort()[-2:][::-1])\n",
    "    preds_all[attrib]=np.array(pred_list.copy())\n",
    "    print()\n",
    "    \n",
    "print(f'Saving to {VERSION}_pred_fashion.csv')\n",
    "test_y_id=[] \n",
    "test_y_predictions=[]\n",
    "for i, itemid in enumerate(fashion_test_df.itemid):\n",
    "    for attrib in fashion_attribs:\n",
    "        test_y_id.append(str(itemid) + f'_{attrib}')\n",
    "        test_y_predictions.append(str(preds_all[attrib][i][0]) + ' ' + str(preds_all[attrib][i][1]))\n",
    "        \n",
    "fashion_result_df = pd.DataFrame(\n",
    "    {'id': test_y_id, 'tagging': test_y_predictions},\n",
    "    columns = ['id', 'tagging'])\n",
    "\n",
    "fashion_result_df.to_csv(f'{VERSION}_pred_fashion.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute: Operating System\n",
      "Loading model...\n",
      "Predicting...\n",
      "Sorting predictions...\n",
      "\n",
      "Attribute: Features\n",
      "Loading model...\n",
      "Predicting...\n",
      "Sorting predictions...\n",
      "\n",
      "Attribute: Network Connections\n",
      "Loading model...\n",
      "Predicting...\n",
      "Sorting predictions...\n",
      "\n",
      "Attribute: Memory RAM\n",
      "Loading model...\n",
      "Predicting...\n",
      "Sorting predictions...\n",
      "\n",
      "Attribute: Brand\n",
      "Loading model...\n",
      "Predicting...\n",
      "Sorting predictions...\n",
      "\n",
      "Attribute: Warranty Period\n",
      "Loading model...\n",
      "Predicting...\n",
      "Sorting predictions...\n",
      "\n",
      "Attribute: Storage Capacity\n",
      "Loading model...\n",
      "Predicting...\n",
      "Sorting predictions...\n",
      "\n",
      "Attribute: Color Family\n",
      "Loading model...\n",
      "Predicting...\n",
      "Sorting predictions...\n",
      "\n",
      "Attribute: Phone Model\n",
      "Loading model...\n",
      "Predicting...\n",
      "Sorting predictions...\n",
      "\n",
      "Attribute: Camera\n",
      "Loading model...\n",
      "Predicting...\n",
      "Sorting predictions...\n",
      "\n",
      "Attribute: Phone Screen Size\n",
      "Loading model...\n",
      "Predicting...\n",
      "Sorting predictions...\n",
      "\n",
      "Saving to Deep_Learning_predictions_pred_mobile.csv\n",
      "CPU times: user 14min 44s, sys: 7.31 s, total: 14min 51s\n",
      "Wall time: 14min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "updated_mobile_title = list(mobile_test_df.title)\n",
    "tokenized_test_titles = mobile_tokenizer.texts_to_sequences(updated_mobile_title)\n",
    "X_test = np.array(tokenized_test_titles)\n",
    "X_test = sequence.pad_sequences(X_test, padding='post', maxlen=max_words_mobile)\n",
    "\n",
    "preds_all={}\n",
    "for attrib in mobile_attribs:\n",
    "    print(f'Attribute: {attrib}')\n",
    "    print(f'Loading model...')\n",
    "    model=load_model(f'{VERSION}_model_mobile_{attrib}.hdf5')\n",
    "    print(f'Predicting...')\n",
    "    pred_attrib=model.predict(X_test)\n",
    "    print(f'Sorting predictions...')\n",
    "    pred_list=[]\n",
    "    for pred in pred_attrib:\n",
    "        pred_list.append(pred.argsort()[-2:][::-1])\n",
    "    preds_all[attrib]=np.array(pred_list.copy())\n",
    "    print()\n",
    "    \n",
    "print(f'Saving to {VERSION}_pred_mobile.csv')\n",
    "test_y_id=[] \n",
    "test_y_predictions=[]\n",
    "for i, itemid in enumerate(mobile_test_df.itemid):\n",
    "    for attrib in mobile_attribs:\n",
    "        test_y_id.append(str(itemid) + f'_{attrib}')\n",
    "        test_y_predictions.append(str(preds_all[attrib][i][0]) + ' ' + str(preds_all[attrib][i][1]))\n",
    "        \n",
    "mobile_result_df = pd.DataFrame(\n",
    "    {'id': test_y_id, 'tagging': test_y_predictions},\n",
    "    columns = ['id', 'tagging'])\n",
    "\n",
    "mobile_result_df.to_csv(f'{VERSION}_pred_mobile.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([beauty_result_df, fashion_result_df, mobile_result_df], axis=0)\n",
    "combined_df.to_csv(f'{VERSION}.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total seconds: 27034.357293\n",
      "Total minutes: 450.57\n",
      "Total hour: 7.51\n"
     ]
    }
   ],
   "source": [
    "duration = datetime.now()-start\n",
    "print('Total seconds:', duration.total_seconds())\n",
    "print(f'Total minutes: {duration.total_seconds()/60:.2f}')\n",
    "print(f'Total hour: {duration.total_seconds()/3600:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
