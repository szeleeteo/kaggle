{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"data/demand-forecasting/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom evaluation metric\n",
    "def SMAPE(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 200.0\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return 'SMAPE', np.nanmean(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape2(y_pred, y_true):\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 200.0\n",
    "    diff = np.abs(y_true - y_pred) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return np.nanmean(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(pred,label):\n",
    "    rmse = np.sqrt(mean_squared_error(pred,label))\n",
    "    mae = mean_absolute_error(pred,label)\n",
    "    smape_score = smape2(pred,label)\n",
    "    \n",
    "    print('RMSE\\t\\t ' + str(rmse))\n",
    "#     print('MAE\\t' + str(mae))\n",
    "    print('SMAPE\\t\\t' + str(smape_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(results,trn,val,metric):\n",
    "    train_errors = results[trn][metric]\n",
    "    validation_errors = results[val][metric]\n",
    "    df = pd.DataFrame([train_errors, validation_errors]).T\n",
    "    df.columns = ['Training', 'Validation']\n",
    "    df.index.name = 'Boosting Round'\n",
    "    ax = df.plot(title=\"XGBoost learning curves\",figsize=(12,5))\n",
    "    ax.set_ylabel(metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(f'{PATH}train.csv', parse_dates=['date'])\n",
    "test = pd.read_csv(f'{PATH}test.csv', parse_dates=['date'], index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['y'] = train['date'].dt.year\n",
    "train['m'] = train['date'].dt.month\n",
    "train['d'] = train['date'].dt.day\n",
    "train['dw'] = train['date'].dt.dayofweek\n",
    "train['dy'] = train['date'].dt.dayofyear\n",
    "train.drop('date', axis=1, inplace=True)\n",
    "\n",
    "sales = train.pop('sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['y'] = test['date'].dt.year\n",
    "test['m'] = test['date'].dt.month\n",
    "test['d'] = test['date'].dt.day\n",
    "test['dw'] = test['date'].dt.dayofweek\n",
    "test['dy'] = test['date'].dt.dayofyear\n",
    "test.drop('date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Training-Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((913000, 7), (913000,), (45000, 7))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, sales.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closest same period (diff year) as test set\n",
    "X_val = train.loc[(train.y==2017) & ((train.m==10) | (train.m==11) | (train.m==12))].copy() \n",
    "y_val = sales[X_val.index].copy()\n",
    "\n",
    "X_train = train.drop(X_val.index).copy()\n",
    "y_train = sales.drop(X_val.index).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((867000, 7), (867000,), (46000, 7), (46000,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "X_val.reset_index(drop=True, inplace=True)\n",
    "y_val.reset_index(drop=True, inplace=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DM_train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "DM_val = xgb.DMatrix(data=X_val, label=y_val)\n",
    "DM_test = xgb.DMatrix(data=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_result = {}\n",
    "watchlist = [(DM_train, \"training\"), (DM_val, \"validation\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_native = {\n",
    "    'objective': 'reg:linear', \n",
    "    'booster':'gbtree',        \n",
    "    'silent': 1,               \n",
    "    'eta': 0.03,                 \n",
    "    'gamma': 0,                 \n",
    "    'max-depth': 3,             \n",
    "    'min_child_weight': 1,      \n",
    "    'max_delta_step': 0,        \n",
    "    'subsample': 0.9,             \n",
    "    'colsample_bytree': 0.7,      \n",
    "    'colsample_bylevel': 0.9,     \n",
    "    'lambda': 0.9,                \n",
    "    'alpha': 0,                 \n",
    "    'scale_pos_weight': 1,      \n",
    "    'base_score': 0.5,          \n",
    "    'eval_metric':'rmse',      \n",
    "    'seed': 42                 \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with partial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttraining-rmse:57.6926\tvalidation-rmse:59.8999\n",
      "Multiple eval metrics have been passed: 'validation-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation-rmse hasn't improved in 20 rounds.\n",
      "[100]\ttraining-rmse:17.7973\tvalidation-rmse:18.623\n",
      "[200]\ttraining-rmse:12.9664\tvalidation-rmse:13.4753\n",
      "[300]\ttraining-rmse:10.9464\tvalidation-rmse:11.3497\n",
      "[400]\ttraining-rmse:9.80133\tvalidation-rmse:10.1634\n",
      "[500]\ttraining-rmse:9.06775\tvalidation-rmse:9.39154\n",
      "[600]\ttraining-rmse:8.56907\tvalidation-rmse:8.89444\n",
      "[700]\ttraining-rmse:8.29426\tvalidation-rmse:8.61039\n",
      "[800]\ttraining-rmse:8.08337\tvalidation-rmse:8.39634\n",
      "[900]\ttraining-rmse:7.94674\tvalidation-rmse:8.26906\n",
      "[1000]\ttraining-rmse:7.81299\tvalidation-rmse:8.14415\n",
      "[1100]\ttraining-rmse:7.73667\tvalidation-rmse:8.07449\n",
      "[1200]\ttraining-rmse:7.67638\tvalidation-rmse:8.02228\n",
      "[1300]\ttraining-rmse:7.61857\tvalidation-rmse:7.96974\n",
      "[1400]\ttraining-rmse:7.56813\tvalidation-rmse:7.92546\n",
      "[1500]\ttraining-rmse:7.53372\tvalidation-rmse:7.89954\n",
      "[1600]\ttraining-rmse:7.49807\tvalidation-rmse:7.87067\n",
      "[1700]\ttraining-rmse:7.46949\tvalidation-rmse:7.8447\n",
      "[1800]\ttraining-rmse:7.44282\tvalidation-rmse:7.82103\n",
      "[1900]\ttraining-rmse:7.41898\tvalidation-rmse:7.80157\n",
      "[2000]\ttraining-rmse:7.39644\tvalidation-rmse:7.78296\n",
      "[2100]\ttraining-rmse:7.37469\tvalidation-rmse:7.76442\n",
      "[2200]\ttraining-rmse:7.36208\tvalidation-rmse:7.75272\n",
      "Stopping. Best iteration:\n",
      "[2276]\ttraining-rmse:7.35087\tvalidation-rmse:7.74291\n",
      "\n",
      "Wall time: 10min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xgb_native = xgb.train(params=params_native, \n",
    "                            dtrain=DM_train,\n",
    "                            num_boost_round=100_000,\n",
    "                            evals=watchlist,\n",
    "                            early_stopping_rounds=20,\n",
    "                            evals_result=evals_result,\n",
    "                            verbose_eval=100)\n",
    "\n",
    "# feval=SMAPE,\n",
    "# maximize=False,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best iteration\t2276\n",
      "Best tree limit\t2277\n",
      "Best RMSE score\t7.742908\n",
      "\n",
      "Validation score\n",
      "RMSE\t\t 7.742944643378778\n",
      "SMAP\t\t12.60422749642427\n",
      "\n",
      "Training score\n",
      "RMSE\t\t 7.349925806207827\n",
      "SMAP\t\t12.780502659751425\n"
     ]
    }
   ],
   "source": [
    "print(\"Best iteration\\t{}\".format(xgb_native.best_iteration))\n",
    "print(\"Best tree limit\\t{}\".format(xgb_native.best_ntree_limit))\n",
    "print(\"Best RMSE score\\t{}\".format(xgb_native.best_score))\n",
    "\n",
    "print(\"\\nValidation score\")\n",
    "pred_val = xgb_native.predict(DM_val)\n",
    "print_scores(pred_val, y_val)\n",
    "\n",
    "print(\"\\nTraining score\")\n",
    "pred_train = xgb_native.predict(DM_train)\n",
    "print_scores(pred_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop with range of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default baseline reference, do not edit\n",
    "params_native_default = {\n",
    "    'objective': 'reg:linear',  # 'reg:linear'\n",
    "    'booster':'gbtree',         # 'gbtree'\n",
    "    'silent': 1,                # 0   \n",
    "    'eta': 0.1,                 # 0.3 alias learning_rate\n",
    "    'gamma': 0,                 # 0\n",
    "    'max-depth': 3,             # 6\n",
    "    'min_child_weight': 1,      # 1\n",
    "    'max_delta_step': 0,        # 0\n",
    "    'subsample': 1,             # 1\n",
    "    'colsample_bytree': 1,      # 1\n",
    "    'colsample_bylevel': 1,     # 1\n",
    "    'lambda': 1,                # 1  \n",
    "    'alpha': 0,                 # 0\n",
    "    'scale_pos_weight': 1,      # 1\n",
    "    'base_score': 0.5,          # 0.5\n",
    "    'eval_metric':'rmse',       # 'rmse' for regression\n",
    "    'seed': 42                  # 0 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_dict = { \n",
    "    'eta':[0.3,0.05],\n",
    "    'max-depth':[4,5,6,7],\n",
    "    'subsample':[0.9,0.8,0.7],\n",
    "    'colsample_bytree':[0.9,0.8,0.7],\n",
    "    'lambda': [0.9,0.8,0.7],              \n",
    "    'alpha': [0.1,0.2],\n",
    "    'gamma': [0.1,0.2],\n",
    "    'colsample_bylevel': [0.9,0.8,0.7,0.6]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "\n",
    "for key,val_list in tuning_dict.items():\n",
    "    for val in val_list:\n",
    "        \n",
    "        print(f\"[[ Iteration {i} - Tuning '{key}' to {val} ]]\")\n",
    "        i+=1\n",
    "        \n",
    "        params_native_copy = params_native_default.copy()\n",
    "        params_native_copy[key] = val\n",
    "\n",
    "        xgb_native_loop = xgb.train(params=params_native_copy, \n",
    "                                    dtrain=DM_train,\n",
    "                                    num_boost_round=100_000,\n",
    "                                    evals=watchlist,\n",
    "                                    early_stopping_rounds=20,\n",
    "                                    evals_result=evals_result,\n",
    "                                    verbose_eval=100)\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"Best iteration\\t{}\".format(xgb_native_loop.best_iteration))\n",
    "        print(\"Best tree limit\\t{}\".format(xgb_native_loop.best_ntree_limit))\n",
    "        print(\"Best RMSE score\\t{}\".format(xgb_native_loop.best_score))\n",
    "\n",
    "        print(\"\\nValidation score\")\n",
    "        pred_val = xgb_native_loop.predict(DM_val)\n",
    "        print_scores(pred_val, y_val)\n",
    "\n",
    "        print(\"\\nTraining score\")\n",
    "        pred_train = xgb_native_loop.predict(DM_train)\n",
    "        print_scores(pred_train, y_train)\n",
    "        \n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred = xgb_native.predict(DM_test, ntree_limit = xgb_native.best_ntree_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Train with full data based on tuned parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "full_data = xgb.DMatrix(data=train, label=sales)\n",
    "xgb_native_full_data = xgb.train(params=params_native, \n",
    "                                dtrain=full_data,\n",
    "                                num_boost_round=1501,\n",
    "                                evals=[(full_data, \"training\")],\n",
    "                                verbose_eval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y_pred = xgb_native_full_data.predict(DM_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(f'{PATH}test.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "submission['sales'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "csv_fn = f'{PATH}tmp/XGB_v3_partial.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "submission[['sales']].to_csv(csv_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "__KAGGLE SCORE: __\n",
    "- 14.27247 (with val)\n",
    "- 14.25877 (full with 1500 boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Dealing with high variance\n",
    "If model is too complex try:\n",
    "- using less features (ie. feature selection),\n",
    "- using more training samples (ie. artificially generated),\n",
    "- increasing regularization (add penalties for extra complexity)\n",
    "\n",
    "In XGBoost you can try to:\n",
    "- reduce depth of each tree (`max_depth`),\n",
    "- increase `min_child_weight` parameter,\n",
    "- increase `gamma` parameter,\n",
    "- add more randomness using `subsample`, `colsample_bytree` parameters,\n",
    "- increase `lambda` and `alpha` regularization parameters\n",
    "\n",
    "#### Dealing with high bias\n",
    "If model is too simple:\n",
    "- add more features (ie. better feature engineering),\n",
    "- more sophisticated model\n",
    "- decrease regularization\n",
    "\n",
    "In XGBoost you can do it by:\n",
    "- increase depth of each tree (`max_depth`),\n",
    "- decrease `min_child_weight` parameter,\n",
    "- decrease `gamma` parameter,\n",
    "- decrease `lambda` and `alpha` regularization parameters\n",
    "\n",
    "Let's try to tweak a parameters a little bit. We are going to add some randomness - each tree we will use 70% randomly chosen samples and 60% randomly chosen features. This should help to reduce a variance. To decrease the bias (bigger accuracy) try adding an extra level to each tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Dealing with high variance\n",
    "If model is too complex try:\n",
    "- using less features (ie. feature selection),\n",
    "- using more training samples (ie. artificially generated),\n",
    "- increasing regularization (add penalties for extra complexity)\n",
    "\n",
    "In XGBoost you can try to:\n",
    "- reduce depth of each tree (`max_depth`),\n",
    "- increase `min_child_weight` parameter,\n",
    "- increase `gamma` parameter,\n",
    "- add more randomness using `subsample`, `colsample_bytree` parameters,\n",
    "- increase `lambda` and `alpha` regularization parameters\n",
    "\n",
    "#### Dealing with high bias\n",
    "If model is too simple:\n",
    "- add more features (ie. better feature engineering),\n",
    "- more sophisticated model\n",
    "- decrease regularization\n",
    "\n",
    "In XGBoost you can do it by:\n",
    "- increase depth of each tree (`max_depth`),\n",
    "- decrease `min_child_weight` parameter,\n",
    "- decrease `gamma` parameter,\n",
    "- decrease `lambda` and `alpha` regularization parameters\n",
    "\n",
    "Let's try to tweak a parameters a little bit. We are going to add some randomness - each tree we will use 70% randomly chosen samples and 60% randomly chosen features. This should help to reduce a variance. To decrease the bias (bigger accuracy) try adding an extra level to each tree."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
